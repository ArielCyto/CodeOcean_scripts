---
title: "Cytoreason Single-cell Analysis Pipeline"
output: html_document
---

```{r setup}
knitr::opts_chunk$set(echo = FALSE)
library(plyr)
library(cytoreason)

devtools::load_all("~/capsule/code/service-single-cell")
devtools::load_all("~/capsule/code/cytoreason.cc.client/")
```

# Overview

* Requirements:
  - a workflow ID from an export single-cell dataset, as output by `sc_api_save_data()`
  - a configuration list that describes the dataset and the analysis models to fit
  - a supported disease model short key, i.e. available in `get_disease_model_metadata()`
  
* The pipeline proceeds as follows:
  - 

# Configuration
```{r config}
config <- sc_api_configuration(
  # asset_id [mandatory]: workflow ID of an exported single-cell dataset as 
  # output by `sc_api_save_data()`
  asset_id = "wf-2ce12cfb61",
  #asset_id = 'wf-a8e301eaec',
  cell_annotation = "biologist_annotation", # or: "final_2023"
  # experiment_id [optional]: experiment identifier.
  # If not provided, the default is to use the asset_id.
  # It does not need to be a GSE ID. 
  # Sample annotations are looked up in the Currator using this key, but the 
  # pipeline will not fail if it does not find any
  experiment_id = "kong_colon",
  
  # platform_id [optional]: platform ID, if not provided it will be "sc-rnaseq" 
  # Note however that the platform ID in the text files used by the platform API will  
  # always be set to "sc-rnaseq".
  platform_id = "sc-rnaseq",
  
  # biosample_id [optional]: a string that indicates the name of the cell annotation
  # variable (in the cell metadata), which holds the bio-sample identifier, 
  # i.e. the key that is used to aggregate the single-cell data into a 
  # pseudobulk data
  biosample_id = "sample_id",
  # biosample_id = "donor_id",
  sample_annotations = read_data("~/capsule/code/UC/Kong study Annotations - SC for CD- updated and used for prod_final_colon.csv",na.strings=''),

  # sample_annotations [optional]:
  # sample_annotations = list(
  #   # condition: values must be from the DOID ontology used in the Curator.
  #   # Here we can remap invalid values in the original data into valid ontology terms.
  #   # Values that are not remapped remain untouched.
  #   # Format: list(<original_variable_name> = c(new_value1 = old_value1, new_value2 = old_value2))
  #   condition = list(
  #       disease = c(control = "normal",
  #                 "Crohn's disease" = "Crohn disease")
  #   ),
  #   # tissue: values must be from the BTO ontology used in the Curator.
  #   # Here we can remap invalid values in the original data into valid ontology terms 
  #   # See example on 'condition' for format description
  #   tissue = list(
  #     tissue = c("colon" = "colon")
  #   ),
  #   # 
  #   sample_classification = list(
  #     type = c("Non inflamed" = "NonI", 
  #              "Inflamed" = "Infl", 
  #              Normal = "Heal")
  #   ),
  #   # directly use column `donor_id` as `subject_id`
  #   subject_id = "donor_id"
  #   # # a custom variable as a cross-product of 2 other variables
  #   # custom_variable = list(
  #   #   type = c("NonI", "Infl", "Heal"),
  #   #   disease = c(healthy = "normal",
  #   #               "CD" = "Crohn disease")
  #   # )
  # ),
    term_metadata = list(
    
    CD_colon = list(
      DZ_vs_HC = list(
        contrast_type = "disease_vs_control"
      )
    ),
    
    INF_VS_Non_INF = list(
      # will only export this contrast term within this comparison
      INF_vs_Non_INF = list(contrast_type = "inflamed_vs_non_inflamed")
    ),
    
    INF_VS_HC = list(
      # will only export this contrast term within this comparison
      INF_vs_HC = list(contrast_type = "inflamed_vs_healthy")
    )
  ),
  analysis_model = list(
    
    CD_colon = list(
      group = list(
        # this will fit the same term as use-case 2, but label it as "DZ_vs_HC"
        condition = c(HC = "healthy", DZ = "Crohn's disease")
      ),
      covariates = "layer"
    ),
    INF_VS_Non_INF = list(
      # group variable: use `sample_classification`, while filtering
      # and enforcing level order
      group = list(
        sample_classification = c(Non_INF = 'Non inflamed',INF = "Inflamed")
      ),
      #,
      # pairing= "donor_id",
      covariates = "layer"
    ),
    INF_VS_HC = list(
      # group variable: use `sample_classification`, while filtering
      # and enforcing level order
      group = list(
        sample_classification = c(HC = "Normal", INF = "Inflamed")
      ),
       covariates = "layer"
    )
  ),
    parameters = list(
    model=list(
      services = c("ct_test","gx_diff"),
      gx_diff=list(min_sample_size=15),
      ct_test=list()
    )
  )
)


```


```{r}
# check the configuration locally
sc_validate_dataset_configuration(config)
# check that a valid dataset can be loaded from the config
pb_object <- prepare_single_cell_dataset(config)
# check a particular analysis model (if failing)
obj <- prepare_single_cell_dataset(config, validate = FALSE)
str(build_analysis_model_metadata(config$analysis_model$CD_colon, obj))
```

## Available model keys
```{r model_keys, echo = FALSE}
knitr::kable(ldply(get_disease_model_metadata(), .id = "model_key", as.data.frame))
```

# Running the pipeline
```{r}
# launch piepline
sc_api_run_analysis("CD-CO", config, tags = list(message = "kong fixed cell names and technical replicates", tissue="colon", analysis="ct test and CSDE"))
# [Thu Mar 14 14:45:19 2024] run_analysis - wf-45a1082950
# [Mon Mar 18 07:44:56 2024] run_analysis - wf-bfcbcd31ca - should be the same as wf-45a1082950
```

```{r}
model_md <- get_disease_model_metadata("CD-CO")
```

```{r}
model_key = "CD-CO"
test_run <- .worker__sc_analysis_pipeline(model_key, config)
```

# BQ upload ----
```{r}
# make_upload_task <- function(wf_id, dataset_name, image, ..., 
#                              memory_request = "500Mi",
#                              task_name = "bq-upload",
#                              data_access = get_workflow(wf_id)[["data_access"]]){
#   if( !test_string(dataset_name, pattern = "^p[0-9xy]+_") ){
#     message("* using BQ dataset: ... ", appendLF = FALSE)
#     dataset_name <- .make_bq_dataset_name(dataset_name, data_access)
#     message(dataset_name)
#     
#   }
#   assert_string(dataset_name, pattern = "^p[0-9xy]+_")
#   assert_string(wf_id)
#   
#   make_task(
#     sprintf("python /app/cytobigquery/exec_service.py single_cell %s %s --verbose -sc -ti=%s -tm=124Mi",
#             # NOTE: the uploader service is working on the old assumption of 2 workflow IDs
#             wf_id,
#             dataset_name,
#             image),
#     # CC config
#     inputs = list(),
#     outdir = "output/",
#     image = image,
#     memory_request = memory_request,
#     task_name = task_name,
#     ...
#   )
#   
# }

combine_ds <- sc_api_save_analysis(c('wf-173f3f7fef','wf-45a1082950'), dataset_name = "p00_cd_colon")#, image = 'eu.gcr.io/cytoreason/ci-cytoreason.single.cell-package:SUP_4341_0.2.0')
# wf-264540eeba
# wf-415d2c44ce - comparison fix also wf-4bdfc1d7d7
# wf-8839a8f065 - check for Yuval - force execution 
# wf-30d79b1a85 - fixed contrast names- V23
# wf-0074db876d - cheking if group b labels in garrid0otrigo is fixed - V24
```

```{r}
library(cytoreason.cc.client)

#########
# image #
#########
task_image = 'eu.gcr.io/cytoreason/cd-py-bigquery:develop_latest'
memory_request = '124Mi'

#####################
# ARGS (positional) #
#####################
service = 'single_cell'
dataset = 'p00_cd_colon'
text_wf_id = 'wf-30d79b1a85'


###################
# ARGS (optional) #
###################
verbose = '--verbose' 
ti = sprintf('-ti=%s', task_image) 
tm = sprintf('-tm=%s', memory_request)


###########
# command #
###########
command <- sprintf('python /app/cytobigquery/exec_service.py %s %s %s %s %s %s', 
                   service, text_wf_id, dataset, verbose, ti, tm)

############
# workflow #
############
tags=list(list(name="de-process", value='bigquery-upload'),
          list(name="service", value=service),
          list(name="export_workflow", value=text_wf_id),
          list(name="target_dataset", value=dataset) 
          )
task_env_vars = list(list("name"="DE_PROCESS","value"="BigQuery_upload") 
                     )
wf <- run_command_dist(command, 
                       outdir = "./output/", 
                       image = task_image , 
                       task_env_vars = task_env_vars,
                       tags=tags,
                       memory_request = memory_request,
                       replace_image_tags = TRUE)

# wf-2c38cae969
```
